import streamlit as st
import requests
import re
import logging
from bs4 import BeautifulSoup
from googlesearch import search
from urllib.robotparser import RobotFileParser
from urllib.parse import urlparse, urljoin
import os

# Setup logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Regex for email validation
EMAIL_REGEX = re.compile(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+")

# File to store blacklisted URLs
BLACKLIST_FILE = "blacklist.txt"

def load_blacklist():
    default_blacklist = {"https://www.myus.com/about/contact/", "https://mobile.yoox.com/customercare/contact-us"}
    if os.path.exists(BLACKLIST_FILE):
        with open(BLACKLIST_FILE, "r") as f:
            return default_blacklist | set(line.strip() for line in f if line.strip())
    return default_blacklist

def update_blacklist(url, blacklist):
    if url not in blacklist:
        with open(BLACKLIST_FILE, "a") as f:
            f.write(url + "\n")
        blacklist.add(url)
        st.info(f"URL added to blacklist: {url}")
        logger.info(f"URL added to blacklist: {url}")

blacklist = load_blacklist()

@st.cache_data
def get_cached_emails(domain, location):
    return set()

@st.cache_data
def cache_emails(domain, location, emails):
    cached_emails = get_cached_emails(domain, location)
    cached_emails.update(emails)
    return cached_emails

def extract_emails_from_text(text):
    return set(match.group() for match in EMAIL_REGEX.finditer(text))

def can_scrape(url):
    parsed = urlparse(url)
    robots_url = urljoin(f"{parsed.scheme}://{parsed.netloc}", "robots.txt")
    rp = RobotFileParser()
    rp.set_url(robots_url)
    try:
        rp.read()
    except Exception as e:
        st.warning(f"Error reading robots.txt from {robots_url}: {e}")
        logger.exception(f"robots.txt error for {robots_url}: {e}")
        return False
    return rp.can_fetch("*", url)

def scrape_emails_from_url(url, domain, location):
    emails = set()
    if url in blacklist:
        st.info(f"Skipping blacklisted URL: {url}")
        return emails
    if not can_scrape(url):
        st.info(f"Scraping disallowed by robots.txt for: {url}")
        return emails
    try:
        response = requests.get(url, timeout=18)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            text = soup.get_text()
            emails = extract_emails_from_text(text)
            if emails:
                cache_emails(domain, location, emails)
    except requests.exceptions.Timeout as e:
        st.warning(f"Timeout fetching {url}: {e}. Adding to blacklist.")
        update_blacklist(url, blacklist)
    except Exception as e:
        st.warning(f"Error fetching {url}: {e}. Adding to blacklist.")
        update_blacklist(url, blacklist)
    return emails

def main():
    st.title("Email Web Scraper - Advanced Version")
    
    email_domain_filter = st.text_input("Enter email domain filter (e.g. @gmail.com):", "@gmail.com")
    country_filter = st.text_input("Enter country to filter results (e.g. USA):", "")
    query = st.text_input("Enter your search query:", "contact email")
    num_emails_needed = st.number_input("How many emails do you need?", min_value=1, value=10, step=1)
    max_urls = st.number_input("Maximum number of URLs to scrape:", min_value=1, value=20, step=1)
    
    final_query = f"{query} {country_filter}" if country_filter.strip() else query
    
    if st.button("Scrape Emails"):
        found_emails = get_cached_emails(email_domain_filter, country_filter)
        
        if len(found_emails) < num_emails_needed:
            try:
                for url in search(final_query, tld="com", lang="en", num=max_urls, stop=max_urls, pause=2):
                    st.write(f"Checking: {url}")
                    emails = scrape_emails_from_url(url, email_domain_filter, country_filter)
                    if emails:
                        found_emails.update(emails)
                    if len(found_emails) >= num_emails_needed:
                        break
            except Exception as e:
                st.error(f"An error occurred during the search: {e}")
                logger.exception(f"Search error: {e}")
        
        found_emails = list(found_emails)[:num_emails_needed]
        
        if found_emails:
            st.success(f"Found {len(found_emails)} email(s):")
            st.text_area("Emails (copy them below):", value="\n".join(found_emails), height=200)
        else:
            st.info("No emails found. Try a different query or adjust parameters.")

main()
